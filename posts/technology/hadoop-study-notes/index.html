<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" type="text/css" href="http://blog.atomd.me/theme/css/style.css"> 
    <link rel="stylesheet" type="text/css" href="http://blog.atomd.me/theme/css/pygments.css">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href="/" type="application/atom+xml" rel="alternate" title="The Other Shore ATOM Feed" />
	
			
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type= "text/javascript">
       MathJax.Hub.Config({
           config: ["MMLorHTML.js"],
           jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML"],
           TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"], equationNumbers: { autoNumber: "AMS" } },
           extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
           tex2jax: { 
               inlineMath: [ ['$','$'] ],
               displayMath: [ ['$$','$$'] ],
               processEscapes: true },
           "HTML-CSS": {
               styles: { ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
           }
       });
    </script>

	
            <title>Hadoop学习笔记 - The Other Shore</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="atomd">
    </head>

<body>
    <section id="navbar">
        <div class="user_meta">
            <h1 id="user"><a href="http://blog.atomd.me" class="">atomd</a></h1>
            <h2>Programmer • Writer</h2>
                        </div>
        <div class="nav-button">
            <ul>
                                                                                                                                        <li><a href="https://github.com/atomd">github</a></li>
                                                <li><a href="https://bitbucket.org/atomd">bitbucket</a></li>
                                                <li><a href="http://www.douban.com/people/atomd/">douban</a></li>
                                 </ul>
         </div>
     </section>

     <section id="sidebar">
        <div class="user_meta">
            <h1 id="user"><a href="http://blog.atomd.me" class="">atomd</a></h1>
            <h2>Programmer • Writer</h2>
            <ul>
                                                                                                            <li><a href="https://github.com/atomd">github</a></li>
                                        <li><a href="https://bitbucket.org/atomd">bitbucket</a></li>
                                        <li><a href="http://www.douban.com/people/atomd/">douban</a></li>
                            </ul>
        </div>
        <footer>
            <address>
                Powered by <a href='http://docs.getpelican.com/en/latest/'>Pelican</a>,
                <a href='https://github.com/jamescooke/pelican-svbtle#readme'>theme info</a>.
            </address>
        </footer>
    </section>

    <section id="posts">
        <article>
    <h1 class="title">
        <a href="http://blog.atomd.me/posts/technology/hadoop-study-notes/" rel="bookmark">Hadoop学习笔记</a>
    </h1>

    <div class='article-content'>
        <p>因为实验室的项目要用到Hadoop，参加了ChinaHadoop办的Hadoop大数据平台的在线培训。用的是YY的平台，第一次用感觉问题不少，尤其是其中的白板功能。我觉得不管上课还是自学都取决于交流的质量。如果真的有富有经验的人愿意传道授业解惑，并且没有交流障碍，无疑会有很大的帮助，也会避免走很多弯路，这样的课是用很大价值的，完全值得300的价格。这里也推荐下这个课:)</p>
<h3>虚拟机配置和相关软件</h3>
<table>
<thead>
<tr>
<th align="center">机器角色</th>
<th align="center">IP</th>
<th align="center">机器名</th>
<th align="center">用户名</th>
<th align="center">配置</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Master + Slave1</td>
<td align="center">192.168.32.111</td>
<td align="center">X001</td>
<td align="center">hadoop</td>
<td align="center">内存1G，硬盘20G</td>
</tr>
<tr>
<td align="center">Slave2</td>
<td align="center">192.168.32.112</td>
<td align="center">X002</td>
<td align="center">hadoop</td>
<td align="center">内存724M, 硬盘20G</td>
</tr>
<tr>
<td align="center">Slave3</td>
<td align="center">192.168.32.113</td>
<td align="center">X003</td>
<td align="center">hadoop</td>
<td align="center">内存724M，硬盘20G</td>
</tr>
</tbody>
</table>
<ul>
<li>VMware Workstation 9.0</li>
<li>ubuntu-12.04.1-desktop-i386.iso</li>
<li>hadoop: hadoop-1.0.4.tar.gz</li>
<li>Java JDK: jdk-7u7-linux-i586.tar.gz</li>
</ul>
<h3>Hadoop集群的安装和配置</h3>
<h4>创建虚拟机X001</h4>
<h5>配置网络</h5>
<ul>
<li>将网卡配置NET的方式。这样虚拟机的网络环境不随宿主机的网络环境变化，而且可以和外网连接。</li>
<li>配置IP 192.168.32.11，掩码 255.255.255.0，网关 192.168.32.2, DNS 192.168.32.2。</li>
</ul>
<h5>更新软件</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo apt-get update
<span class="nv">$ </span>sudo apt-get upgrade
</pre></div>


<h5>安装SSH</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo apt-get install ssh
</pre></div>


<h5>修改配置文件</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo vim /etc/ssh/ssh_config
StrictHostKeyChecking no

<span class="nv">$ </span>sudo service ssh restart
</pre></div>


<h5>修改hostname为X001</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo vi /etc/hostname
X001
</pre></div>


<h5>修改hosts</h5>
<div class="highlight"><pre><span class="nv">$ </span>vim /etc/hosts

127.0.0.1   localhost
127.0.1.1   X001-local
192.168.1.111   X001
192.168.1.112   X002
192.168.1.113   X003
</pre></div>


<h5>下载hadoop和Java</h5>
<ul>
<li>hadoop: hadoop-1.0.4.tar.gz</li>
<li>Java JDK: jdk-7u7-linux-i586.tar.gz</li>
</ul>
<h5>如果通过ftp上传可以安装vsftpd：</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo apt-get install vsftpd
<span class="nv">$ </span>sudo vi /etc/vsftpd.conf

<span class="nv">listen</span><span class="o">=</span>YES
<span class="nv">anonymous_enable</span><span class="o">=</span>NO
<span class="nv">local_enable</span><span class="o">=</span>YES
<span class="nv">write_enable</span><span class="o">=</span>YES
<span class="nv">local_umask</span><span class="o">=</span>022
<span class="nv">anon_upload_enable</span><span class="o">=</span>YES
<span class="nv">dirmessage_enable</span><span class="o">=</span>YES
<span class="nv">use_localtime</span><span class="o">=</span>YES
<span class="nv">xferlog_enable</span><span class="o">=</span>YES
<span class="nv">connect_from_port_20</span><span class="o">=</span>YES
<span class="nv">async_abor_enable</span><span class="o">=</span>YES
<span class="nv">ascii_upload_enable</span><span class="o">=</span>YES
<span class="nv">ascii_download_enable</span><span class="o">=</span>YES
<span class="nv">secure_chroot_dir</span><span class="o">=</span>/var/run/vsftpd/empty
<span class="nv">pam_service_name</span><span class="o">=</span>vsftpd
<span class="nv">rsa_cert_file</span><span class="o">=</span>/etc/ssl/private/vsftpd.pem

<span class="nv">$ </span>sudo service vsftpd restart
</pre></div>


<h4>安装Java</h4>
<h5>切换为root用户</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo su -
</pre></div>


<h5>解压到/usr/java目录下</h5>
<div class="highlight"><pre><span class="nv">$ </span>tar -xvf /usr/java/jdk-7u7-linux-i586.tar.gz
</pre></div>


<h5>设置JAVA环境变量，修改/etc/profile，在文件尾部加入</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo vim /etc/profile

<span class="c"># set java env</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/java/jdk1.7.0_07
<span class="nb">export </span><span class="nv">JRE_HOME</span><span class="o">=</span>/usr/java/jdk1.7.0_07/jre
<span class="nb">export </span><span class="nv">CLASSPATH</span><span class="o">=</span>.:<span class="nv">$CLASSPATH</span>:<span class="nv">$JAVA_HOME</span>/lib:<span class="nv">$JRE_HOME</span>/lib
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$JAVA_HOME</span>/bin:<span class="nv">$JRE_HOME</span>/bin
</pre></div>


<h5>测试安装</h5>
<div class="highlight"><pre><span class="nv">$ </span><span class="nb">source</span> /etc/profile
<span class="nv">$ </span>java -version
</pre></div>


<h4>安装Hadoop</h4>
<h5>切换为hadoop用户</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo su hadoop
</pre></div>


<h5>解压在目录/usr/hadoop-1.0.4</h5>
<div class="highlight"><pre><span class="nv">$ </span>tar -xvf /usr/hadoop-1.0.4.tar.gz
</pre></div>


<h5>切换为root用户</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo su -
</pre></div>


<h5>修改/etc/profile，设置Hadoop环境变量</h5>
<div class="highlight"><pre><span class="nv">$ </span>sudo vim /etc/profile

<span class="c"># set hadoop env</span>
<span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span>/usr/hadoop-1.0.4
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$HADOOP_HOME</span>/bin
<span class="nb">export </span><span class="nv">HADOOP_HOME_WARN_SUPPRESS</span><span class="o">=</span>1
</pre></div>


<h5>使环境生效</h5>
<div class="highlight"><pre><span class="nv">$ </span><span class="nb">source</span> /etc/profile
</pre></div>


<h4>hadoop的基本配置(/usr/hadoop-1.0.4/conf)</h4>
<div class="highlight"><pre><span class="nv">$ </span>vim hadoop-env.sh
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/java/jdk1.7.0_07
</pre></div>


<h5>修改core-site.xml，tmp.dir不要放在/tmp下</h5>
<div class="highlight"><pre><span class="nv">$ </span>vim core-site.xml

&lt;configuration&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/usr/hadoop-1.0.4/tmp.dir&lt;/value&gt;
    &lt;description&gt;A base <span class="k">for </span>other temporary dirs.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://192.168.32.111:9000&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</pre></div>


<h5>修改 hdfs-site.xml， permision为false是为了开发方便，replication副本数为2（缺省值为3），因为测试环境中一共是3个节点。</h5>
<div class="highlight"><pre><span class="nv">$ </span>vim hdfs-site.xml

&lt;configuration&gt;
&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.permissions&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</pre></div>


<h5>修改mapred-site.xml</h5>
<div class="highlight"><pre><span class="nv">$ </span>vim mapred-site.xml
&lt;configuration&gt;
&lt;property&gt;
    &lt;name&gt;mapred.job.tracker&lt;/name&gt;
    &lt;value&gt;http://192.168.32.111:9001&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</pre></div>


<h5>修改masters和slaves文件</h5>
<div class="highlight"><pre><span class="nv">$ </span>cat masters
192.168.32.111

<span class="nv">$ </span>cat slaves
192.168.32.111
192.168.32.112
192.168.32.113
</pre></div>


<h4>通过vmware 克隆出X002和X003</h4>
<h5>按照X001的方式对Ip、 Hostname、Hosts进行修改</h5>
<h5>分配对三个节点配置SSH无密码登陆</h5>
<div class="highlight"><pre><span class="nv">$ </span>ssh-keygen -t rsa -f ~/.ssh/id_rsa -P <span class="s2">&quot;&quot;</span>
<span class="nv">$ </span>ssh-copy-id hadoop@X001
<span class="nv">$ </span>ssh-copy-id hadoop@X002
<span class="nv">$ </span>ssh-copy-id hadoop@X003
</pre></div>


<p>这样就完成3个节点的hadoop的测试环境的搭建</p>
<h3>测试基本功能</h3>
<h4>在master机器上，格式化HDFS文件系统</h4>
<div class="highlight"><pre>hadoop@X001:~<span class="nv">$ </span>hadoop namenode -format
...
1.0.4/tmp.dir/dfs/name has been successfully formatted.
...
</pre></div>


<h4>启动Hadoop</h4>
<div class="highlight"><pre>hadoop@X001:~<span class="nv">$ </span>start-all.sh

starting namenode, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-namenode-X001.out
192.168.32.112: starting datanode, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-datanode-X002.out
192.168.32.113: starting datanode, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-datanode-X003.out
192.168.32.111: starting datanode, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-datanode-X001.out
192.168.32.111: starting secondarynamenode, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-secondarynamenode-X001.out
starting jobtracker, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-jobtracker-X001.out
192.168.32.112: starting tasktracker, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-tasktracker-X002.out
192.168.32.113: starting tasktracker, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-tasktracker-X003.out
192.168.32.111: starting tasktracker, logging to /usr/hadoop-1.0.4/libexec/../logs/hadoop-hadoop-tasktracker-X001.out
</pre></div>


<h4>查看启动进程</h4>
<div class="highlight"><pre>hadoop@X001:~<span class="nv">$ </span>jps
7740 DataNode
7516 NameNode
8427 Jps
8271 TaskTracker
7956 SecondaryNameNode
8039 JobTracker

hadoop@X002:~<span class="nv">$ </span>jps
4722 DataNode
4960 Jps
4915 TaskTracker
</pre></div>


<h4>通过Web查看节点状态</h4>
<p>访问http://192.168.32.111:50030 或 http://192.168.32.111:50070 来查看集群的相关信息</p>
<h3>从集群删除节点</h3>
<p><strong>在主节点即namenode进行操作</strong></p>
<h4>修改conf/hdfs-site.xml文件</h4>
<div class="highlight"><pre><span class="nv">$ </span>vim hdfs-site.xml

&lt;property&gt;
        &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;
        &lt;value&gt;/usr/hadoop-1.0.4/conf/excludes&lt;/value&gt;
        &lt;final&gt;true&lt;/final&gt;
&lt;/property&gt;
</pre></div>


<h4>在excludes文件中加入X002</h4>
<div class="highlight"><pre><span class="nv">$ </span>cat excludes
X002
</pre></div>


<h4>查看当前报告，可以发现所有节点的状态都是Normal</h4>
<div class="highlight"><pre><span class="nv">$ </span>hadoop dfsadmin -report

...
-------------------------------------------------
Datanodes available: 3 <span class="o">(</span>3 total, 0 dead<span class="o">)</span>

Name: 192.168.32.112:50010
Decommission Status : Normal
...

Name: 192.168.32.113:50010
Decommission Status : Normal
...

Name: 192.168.32.111:50010
Decommission Status : Normal
...
</pre></div>


<h4>执行refreshNodes命令</h4>
<p>可以动态刷新dfs.hosts和dfs.hosts.exclude配置，无需重启NameNode</p>
<div class="highlight"><pre><span class="nv">$ </span>hadoop dfsadmin -report

...
-------------------------------------------------
Datanodes available: 2 <span class="o">(</span>3 total, 1 dead<span class="o">)</span>

Name: 192.168.32.113:50010
Decommission Status : Normal
...

Name: 192.168.32.111:50010
Decommission Status : Normal
...

Name: 192.168.32.112:50010
Decommission Status : Decommissioned
...
</pre></div>


<h4>查看当前报告，观察X002节点的状态</h4>
<p>可以发现X002节点的状态变为了Decommission in progress，等到当要删除的节点状态变为Decommission Status : Decommissioned 时，数据已经备份，并且完成删除操作。</p>
<p><strong>在删除节点执行操作</strong></p>
<p>refreshNode会停掉datanode，但是没有停掉tasktracker,所以需要执行</p>
<div class="highlight"><pre><span class="nv">$ </span>hadoop-daemon.sh stop tasktracker
</pre></div>


<h3>添加节点，进行负载均衡</h3>
<h4>该节点上清空dfs/data/current 目录下文件</h4>
<p>为负载重新平衡做准备</p>
<h4>从excludes文件中移除，再次refreshNodes</h4>
<div class="highlight"><pre><span class="nv">$ </span>vim excludes
<span class="nv">$ </span>hadoop dfsadmin -refreshNodes
</pre></div>


<h4>在X002节点上启动datanode和tasktracker</h4>
<div class="highlight"><pre>hadoop@X002:/usr/hadoop-1.0.4<span class="nv">$ </span>hadoop-daemon.sh start datanode
hadoop@X002:/usr/hadoop-1.0.4<span class="nv">$ </span>hadoop-daemon.sh start tasktracker
</pre></div>


<h4>等到主节点检测新增节点X002后，完成负载均衡</h4>
<div class="highlight"><pre><span class="nv">$ </span>start-balancer.sh
//wait ... ...
<span class="nv">$ </span>tail logs/hadoop-hadoop-balancer-X002.out
...
The cluster is balanced. Exiting...
Balancing took 2.651883333333333 minutes
</pre></div>
    </div>

    <div class='post-footer'>
        <div class='share-footer'>
            <div id='dated' class="abbr published" title='2013-01-13T19:03:00'>
                Posted Sun, 13 Jan 13
            </div>

             
        </div>

        <div id="article_meta">
                        Category:
            <a href="http://blog.atomd.me/category/technology/">technology</a>
                                    <br />Tags:
                        <a href="http://blog.atomd.me/tag/hadoop/">hadoop</a>
                        <a href="http://blog.atomd.me/tag/installation/">installation</a>
                        <a href="http://blog.atomd.me/tag/configuration/">configuration</a>
                                </div>

        
    </div>
</article>

<footer>
    <a href="http://blog.atomd.me/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
</footer>

<div id="comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
    var disqus_identifier = "posts/technology/hadoop-study-notes/";
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://atomdblog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view <a href="http://disqus.com/?ref_noscript">comments</a>.</noscript>
</div>
    </section>

    <script type="text/javascript">
	var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
	document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
	</script>
	<script type="text/javascript">
		try {
			var pageTracker = _gat._getTracker("UA-35007756-1");
			pageTracker._trackPageview();
		} catch(err) {}</script>
</body>
</html>